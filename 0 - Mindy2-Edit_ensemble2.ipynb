{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c50f01a-72fd-4804-9f54-d77b52d3991f",
   "metadata": {
    "id": "6c50f01a-72fd-4804-9f54-d77b52d3991f"
   },
   "source": [
    "# Pangyo AI Challenge 2021 - Mask Classification Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b",
   "metadata": {
    "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b"
   },
   "source": [
    "## 라이브러리 호출 및 I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a",
   "metadata": {
    "executionInfo": {
     "elapsed": 5126,
     "status": "ok",
     "timestamp": 1630047280444,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a"
   },
   "outputs": [],
   "source": [
    "import os, torch, copy, cv2, sys, random, logging\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchcontrib.optim import SWA\n",
    "import torchcontrib\n",
    "from torch.optim.swa_utils import SWALR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f99a98c-43cb-4edd-b6ff-6c7b7cfa3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6AYoEZyP1V_V",
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1630047300749,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "6AYoEZyP1V_V"
   },
   "outputs": [],
   "source": [
    "def get_logger(name: str, file_path: str, stream=False) -> logging.RootLogger:\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(file_path)\n",
    "\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    if stream:\n",
    "        logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954",
   "metadata": {
    "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954"
   },
   "source": [
    "## Argument Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9c4250-2257-404f-941d-58eff1e9eb38",
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1630047303454,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "8f9c4250-2257-404f-941d-58eff1e9eb38"
   },
   "outputs": [],
   "source": [
    "# 시드(seed) 설정\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#seed_everything(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9",
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1630047307151,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9"
   },
   "outputs": [],
   "source": [
    "# working directory 지정\n",
    "ROOT_PATH = './'\n",
    "DATA_DIR = os.path.join(ROOT_PATH, 'pangyo_ai', 'task01_mask', 'train')\n",
    "RESULT_DIR = os.path.join(ROOT_PATH, 'task01_mask_results')\n",
    "if not os.path.isdir(RESULT_DIR):\n",
    "    os.makedirs(RESULT_DIR)\n",
    "\n",
    "# hyper-parameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "INPUT_SHAPE = (240, 240)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44807b0-7788-49ec-aff2-c756e4513c5e",
   "metadata": {
    "id": "d44807b0-7788-49ec-aff2-c756e4513c5e"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05",
   "metadata": {
    "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05"
   },
   "source": [
    "#### Train & Validation Set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04642777-c2e0-439b-9692-f6c571a86521",
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1630047309743,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "04642777-c2e0-439b-9692-f6c571a86521"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, input_shape, augment=False, augtype='imagenet'):\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        self.augment = augment\n",
    "        \n",
    "        if os.path.isfile(os.path.join(RESULT_DIR, 'total.pkl')):\n",
    "            self.db = pd.read_pickle(os.path.join(RESULT_DIR, 'total.pkl'))\n",
    "        else:\n",
    "            self.db = self.data_loader()\n",
    "            self.db = self.db.sample(frac=1).reset_index()\n",
    "            self.db.to_pickle(os.path.join(RESULT_DIR, 'total.pkl'))\n",
    "            \n",
    "        if self.mode == 'train':\n",
    "            self.db = self.db[:int(len(self.db) * 0.9)]\n",
    "        elif self.mode == 'val':\n",
    "            self.db = self.db[int(len(self.db) * 0.9):]\n",
    "            self.db.reset_index(inplace=True)\n",
    "        else:\n",
    "            print(f'!!! Invalid split {self.mode}... !!!')\n",
    "        \n",
    "        policies = [\n",
    "            transforms.AutoAugmentPolicy.CIFAR10, \n",
    "            transforms.AutoAugmentPolicy.IMAGENET, \n",
    "            transforms.AutoAugmentPolicy.SVHN\n",
    "        ]\n",
    "        dic = {'cifar' : 0, 'imagenet' : 1, 'svhn' :2}\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.input_shape), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.ConvertImageDtype(torch.uint8),\n",
    "            transforms.AutoAugment(policies[dic[augtype]]),\n",
    "        ])\n",
    "        self.transform2 = transforms.Compose([\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.transform3 = transforms.Compose([\n",
    "            transforms.Resize(self.input_shape), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "            \n",
    "        mask_image_list = os.listdir(os.path.join(self.data_dir, 'Mask'))\n",
    "        nomask_image_list = os.listdir(os.path.join(self.data_dir, 'NoMask'))\n",
    "        mask_image_list = [item for item in mask_image_list if item[-4:] == '.png']\n",
    "        nomask_image_list = [item for item in nomask_image_list  if item[-4:] == '.png']\n",
    "        mask_image_path = list(map(lambda x : os.path.join(self.data_dir, 'Mask', x), mask_image_list))\n",
    "        nomask_image_path = list(map(lambda x : os.path.join(self.data_dir, 'NoMask', x), nomask_image_list))\n",
    "\n",
    "        # encoding label (Mask : 1, No Mask : 0)\n",
    "        mask_df = pd.DataFrame({'img_path':mask_image_path, 'label':np.ones(len(mask_image_list))})\n",
    "        nomask_df = pd.DataFrame({'img_path':nomask_image_path, 'label':np.zeros(len(nomask_image_list))})\n",
    "        db = mask_df.append(nomask_df, ignore_index=True)\n",
    "        return db\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = Image.fromarray(cvimg)\n",
    "        if self.augment == True:\n",
    "            trans_image = self.transform(trans_image)\n",
    "            trans_image = self.transform2(trans_image)\n",
    "        else:\n",
    "            trans_image = self.transform3(trans_image)\n",
    "            \n",
    "        trans_image = TF.crop(trans_image, 0, 0, 240, 240)\n",
    "\n",
    "        return trans_image, data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d056905-1f77-4579-a260-07bb1056f6db",
   "metadata": {
    "id": "1d056905-1f77-4579-a260-07bb1056f6db"
   },
   "source": [
    "## EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b4c3315-ebca-4e6b-a8f2-1281ccd0bb87",
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1630047327958,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "1b4c3315-ebca-4e6b-a8f2-1281ccd0bb87"
   },
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int, verbose: bool, logger:logging.RootLogger=None)-> None:\n",
    "        \"\"\" 초기화\n",
    "\n",
    "        Args:\n",
    "            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "            weight_path (str): weight 저장경로\n",
    "            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.logger = logger\n",
    "        self.stop = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\n",
    "\n",
    "        Args:\n",
    "            loss (float):\n",
    "\n",
    "        Examples:\n",
    "            \n",
    "        Note:\n",
    "            \n",
    "        \"\"\"  \n",
    "\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "            # self.save_checkpoint(loss=loss, model=model)\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopper, Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "\n",
    "            if self.verbose:\n",
    "                self.logger.info(msg) if self.logger else print(msg)\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            self.save_model = True\n",
    "            msg = f\"Early stopper, Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "            # self.save_checkpoint(loss=loss, model=model)\n",
    "\n",
    "            if self.verbose:\n",
    "                self.logger.info(msg) if self.logger else print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaffd8d-b025-42c1-8dd8-69529487389e",
   "metadata": {
    "id": "1aaffd8d-b025-42c1-8dd8-69529487389e"
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5faaac1b-64c3-4659-82de-d4309502f29a",
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1630047330626,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "5faaac1b-64c3-4659-82de-d4309502f29a"
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion, model, device, metric_fn, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.criterion = criterion\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            loss = self.criterion(pred, label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "        return epoch_index, self.train_mean_loss, self.train_score, auroc\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            ## coordinate loss\n",
    "            loss = self.criterion(pred, label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "        return self.val_mean_loss, self.validation_score, auroc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aca506-d168-4c9f-8eca-5cdecb122961",
   "metadata": {
    "id": "e2aca506-d168-4c9f-8eca-5cdecb122961"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33678d90-a254-48d5-bf09-2a817eeafea3",
   "metadata": {
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1630047333382,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "33678d90-a254-48d5-bf09-2a817eeafea3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def get_metric_fn(y_pred, y_answer, y_prob):\n",
    "    \"\"\" 성능을 반환하는 함수\"\"\"\n",
    "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
    "    accuracy = accuracy_score(y_answer, y_pred)\n",
    "    auroc = roc_auc_score(y_answer, y_prob)\n",
    "    return accuracy, auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729c079-9d85-49ce-857f-320b0c56a3a8",
   "metadata": {
    "id": "d729c079-9d85-49ce-857f-320b0c56a3a8",
    "tags": []
   },
   "source": [
    "## 학습을 위한 객체 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cea68f0-dfad-47ce-a8ca-00ea01988886",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1630047464030,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "7cea68f0-dfad-47ce-a8ca-00ea01988886",
    "outputId": "79f0de07-a86a-45fb-d1c3-d4a3c835001e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set samples: 48776 Val set samples: 2710\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "original_dataset = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE)\n",
    "imagenet = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE, augment = True, augtype='imagenet')\n",
    "cifar10 = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE, augment = True, augtype='cifar')\n",
    "svhn = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE, augment = True, augtype='svhn')\n",
    "\n",
    "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val', input_shape=INPUT_SHAPE)\n",
    "\n",
    "Dset = [original_dataset, imagenet]\n",
    "train_dataset = torch.utils.data.ConcatDataset(Dset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print('Train set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a41fac-63d4-4f77-a06f-485af2d27e45",
   "metadata": {
    "id": "61b27520-c82c-4ec8-ae0b-119a79167f09"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "734b37a9-71b0-4623-a304-d8b6cf9e8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from vit_pytorch import ViT\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dfc9c7c-664c-4e48-8eb2-8b2e0c699eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Multi-GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71a9433-6baf-45af-b5aa-897e18dc7b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Predict with ensemble models\n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, nb_classes=2):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.modelA = timm.create_model(\n",
    "            'vit_large_patch16_224',\n",
    "            pretrained=True, \n",
    "            img_size=(240, 240),\n",
    "            num_classes=2\n",
    "        )\n",
    "        self.modelB = timm.create_model(\n",
    "            'eca_nfnet_l1', \n",
    "            pretrained=True\n",
    "        )\n",
    "        \n",
    "        # Remove last linear layer\n",
    "        n_features_A = self.modelA.head.in_features\n",
    "        self.modelA.head = nn.Identity()\n",
    "        \n",
    "        n_features_B = self.modelB.head.fc.in_features\n",
    "        self.modelB.head.fc = nn.Identity()\n",
    "        \n",
    "        # Create new classifier\n",
    "        self.classifier = nn.Linear(n_features_A + n_features_B, nb_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = self.modelB(x)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        x = self.classifier(F.relu(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b104fb6-e85b-452d-a58d-20226594ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ensemble().to(device)\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# Save Initial Model\n",
    "torch.save({'model':model.state_dict()}, os.path.join(RESULT_DIR, 'initial.pt'))\n",
    "\n",
    "# Set optimizer, scheduler, loss function, metric function\n",
    "#SWA_LR = 0.01\n",
    "#base_opt = optim.SGD(model.parameters(), lr=LEARNING_RATE) #momentum = 0.9, weight_decay = 0.1)\n",
    "#optimizer = torchcontrib.optim.SWA(base_opt)\n",
    "#scheduler = SWALR(optimizer, swa_lr=SWA_LR)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "metric_fn = get_metric_fn\n",
    "\n",
    "# Set system logger\n",
    "system_logger = get_logger(name='train',file_path='train_log.log')\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n",
    "\n",
    "# Set earlystopper\n",
    "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE, verbose=True, logger=system_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02efa36b-ed61-43af-af4f-f27a0faa6502",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 3165,
     "status": "error",
     "timestamp": 1630047482832,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "dcc35f70-25fc-48e1-92f8-8633b3b8be80",
    "outputId": "75823f3f-b65d-4de2-98bc-7dc236a0dae8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-13-2e6d75fc1828>\", line 27, in forward\n    x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 347, in forward\n    x = self.forward_features(x)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 339, in forward_features\n    x = self.blocks(x)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    input = module(input)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 213, in forward\n    x = x + self.drop_path(self.attn(self.norm1(x)))\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 186, in forward\n    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 96, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py\", line 1847, in linear\n    return torch._C._nn.linear(input, weight, bias)\nRuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 1; 5.80 GiB total capacity; 4.18 GiB already allocated; 10.81 MiB free; 4.69 GiB reserved in total by PyTorch)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-538866116584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mepoch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mepoch_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_mean_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0c343f1391c9>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataloader, epoch_index)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-13-2e6d75fc1828>\", line 27, in forward\n    x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 347, in forward\n    x = self.forward_features(x)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 339, in forward_features\n    x = self.blocks(x)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    input = module(input)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 213, in forward\n    x = x + self.drop_path(self.attn(self.norm1(x)))\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 186, in forward\n    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 96, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py\", line 1847, in linear\n    return torch._C._nn.linear(input, weight, bias)\nRuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 1; 5.80 GiB total capacity; 4.18 GiB already allocated; 10.81 MiB free; 4.69 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "criterion = 1E+8\n",
    "epoch_lst = []\n",
    "train_mean_lst = []\n",
    "train_acc_lst = []\n",
    "train_auroc_lst = []\n",
    "\n",
    "val_mean_lst = []\n",
    "val_acc_lst = []\n",
    "val_auroc_lst = []\n",
    "\n",
    "for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "    epoch_index, mean_loss, acc, auroc = trainer.train_epoch(train_dataloader, epoch_index)\n",
    "    epoch_lst.append(epoch_index)\n",
    "    train_mean_lst.append(mean_loss)\n",
    "    train_acc_lst.append(acc)\n",
    "    train_auroc_lst.append(auroc)\n",
    "    \n",
    "    mean_loss, acc, auroc = trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "    val_mean_lst.append(mean_loss)\n",
    "    val_acc_lst.append(acc)\n",
    "    val_auroc_lst.append(auroc)\n",
    "    \n",
    "    # early_stopping check\n",
    "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
    "\n",
    "    if early_stopper.stop:\n",
    "        print('Early stopped')\n",
    "        break\n",
    "\n",
    "    if trainer.val_mean_loss < criterion:\n",
    "        criterion = trainer.val_mean_loss\n",
    "        check_point = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(check_point, os.path.join(RESULT_DIR, 'best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003589b5-7186-48b7-a050-9565f2bde34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_lst, train_mean_lst)\n",
    "plt.plot(epoch_lst, val_mean_lst)\n",
    "plt.title(\"Train & Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epoch_lst, train_acc_lst)\n",
    "plt.plot(epoch_lst, val_acc_lst)\n",
    "plt.title(\"Train & Validation Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epoch_lst, train_auroc_lst)\n",
    "plt.plot(epoch_lst, val_auroc_lst)\n",
    "plt.title(\"Train & Validation AUROC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53514a-e83f-4795-9589-640f26cc2993",
   "metadata": {
    "id": "fe53514a-e83f-4795-9589-640f26cc2993"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729cfde-c4b3-4d36-938e-f8bb8d8afef3",
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1630047487060,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "6729cfde-c4b3-4d36-938e-f8bb8d8afef3"
   },
   "outputs": [],
   "source": [
    "TRAINED_MODEL_PATH = os.path.join(RESULT_DIR, 'best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331",
   "metadata": {
    "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331"
   },
   "source": [
    "#### Test set Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced90de9-50ec-4e18-9f42-5a1b493941a5",
   "metadata": {
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1630047488634,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "ced90de9-50ec-4e18-9f42-5a1b493941a5"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.input_shape), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['label']\n",
    "    \n",
    "    def data_loader(self):\n",
    "        print('Loading ' + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "\n",
    "        mask_image_list = os.listdir(os.path.join(self.data_dir, 'Mask'))\n",
    "        nomask_image_list = os.listdir(os.path.join(self.data_dir, 'NoMask'))\n",
    "        mask_image_list = [item for item in mask_image_list if item[-4:] == '.png']\n",
    "        nomask_image_list = [item for item in nomask_image_list  if item[-4:] == '.png']\n",
    "        mask_image_path = list(map(lambda x : os.path.join(self.data_dir, 'Mask', x), mask_image_list))\n",
    "        nomask_image_path = list(map(lambda x : os.path.join(self.data_dir, 'NoMask', x), nomask_image_list))\n",
    "\n",
    "        # encoding label (Mask : 1, No Mask : 0)\n",
    "        mask_df = pd.DataFrame({'img_path':mask_image_path, 'label':np.ones(len(mask_image_list))})\n",
    "        nomask_df = pd.DataFrame({'img_path':nomask_image_path, 'label':np.zeros(len(nomask_image_list))})\n",
    "        db = mask_df.append(nomask_df, ignore_index=True)\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6127,
     "status": "ok",
     "timestamp": 1630047496452,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
    "outputId": "191d15b8-07ab-4d56-a2af-071ec0ec4b8c"
   },
   "outputs": [],
   "source": [
    "DATA_DIR=os.path.join(ROOT_PATH, 'pangyo_ai', 'task01_mask', 'test_annot')\n",
    "\n",
    "# Load dataset & dataloader\n",
    "test_dataset = TestDataset(data_dir=DATA_DIR, input_shape=INPUT_SHAPE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f771e9d-62bd-4f07-b268-decd9ef5aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We made..\n",
    "if os.path.isfile(os.path.join(DATA_DIR, 'test.pkl')):\n",
    "    test_db = pd.read_pickle(os.path.join(DATA_DIR, 'test.pkl'))\n",
    "else:\n",
    "    test_db = TestDataset.labeling()\n",
    "    test_db.to_pickle(os.path.join(DATA_DIR, 'test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a090ea-bb34-4d3d-a127-b1190e8c416c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 6703,
     "status": "error",
     "timestamp": 1630047520166,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "16a090ea-bb34-4d3d-a127-b1190e8c416c",
    "outputId": "f5f39cdb-76f6-4e4d-f1f5-f7eb8d544c57"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "# Prediction\n",
    "file_num_lst = []\n",
    "pred_lst = []\n",
    "prob_lst = []\n",
    "target_lst = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, label) in enumerate(test_dataloader):\n",
    "        img = img.to(device)\n",
    "        pred = model(img)\n",
    "        #file_num_lst.extend(list(file_num))\n",
    "        pred_lst.extend(pred.argmax(dim=1).tolist())\n",
    "        prob_lst.extend(pred[:, 1].tolist())\n",
    "        target_lst.extend(label.cpu().tolist())\n",
    "\n",
    "acc, roc = get_metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "print('Accuracy: ', acc, ', ', 'AUROC: ', roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d6fc9-e021-4c64-b1ae-80b90681ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.input_shape), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading test dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        image_list = os.listdir(self.data_dir)\n",
    "        image_list = [item for item in image_list if item[-4:] == '.png']\n",
    "        image_path = list(map(lambda x : os.path.join(self.data_dir, x), image_list))\n",
    "        db = pd.DataFrame({'img_path':image_path, 'file_num':list(map(lambda x : x.split('.')[0], image_list))})\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['file_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24223815-1436-418d-8b7f-739373db91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=os.path.join(ROOT_PATH, 'pangyo_ai', 'task01_mask', 'test')\n",
    "\n",
    "# Load dataset & dataloader\n",
    "test_dataset = TestDataset(data_dir=DATA_DIR, input_shape=INPUT_SHAPE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71dbe2-9a22-4e3c-864d-493017cde379",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "# Prediction\n",
    "file_num_lst = []\n",
    "pred_lst = []\n",
    "prob_lst = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, file_num) in enumerate(test_dataloader):\n",
    "        img = img.to(device)\n",
    "        pred = model(img)\n",
    "        file_num_lst.extend(list(file_num))\n",
    "        pred_lst.extend(pred.argmax(dim=1).tolist())\n",
    "        prob_lst.extend(pred[:, 1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056169d1-64a8-4b81-8daf-722b029cf2b9",
   "metadata": {
    "id": "056169d1-64a8-4b81-8daf-722b029cf2b9"
   },
   "source": [
    "#### Save results as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133cd86-b87b-4f8b-ae0e-c240655ae9ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "error",
     "timestamp": 1629972128063,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "f133cd86-b87b-4f8b-ae0e-c240655ae9ff",
    "outputId": "d8861db3-cdd0-4ce3-ebb4-9decbe1145e5"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file_name':list(map(int,file_num_lst)), 'answer':pred_lst, 'prob':prob_lst})\n",
    "df.sort_values(by=['file_name'], inplace=True)\n",
    "df.to_csv(os.path.join(RESULT_DIR, 'mask_pred.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3026cec-10b7-4a5b-be8d-f0605fe07279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01_mask_code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "023f93d14c934299be94113aa089ff2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c01333330ae4863af15c0057fc3ae56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71dbfe7632524d69b167b4d56d44ed41",
      "placeholder": "​",
      "style": "IPY_MODEL_ef0ccb555eb34b45a4c5af40649f68ce",
      "value": "100%"
     }
    },
    "2e1f1639316a49e4acccba120fb254d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c01333330ae4863af15c0057fc3ae56",
       "IPY_MODEL_48d64e6e8d294ebd974fd9e610e935d0",
       "IPY_MODEL_bb54549aef364adca5e067690cf7258c"
      ],
      "layout": "IPY_MODEL_c972d82225b84aa3aa6cb513952162b9"
     }
    },
    "48d64e6e8d294ebd974fd9e610e935d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce8f3ea6a7de46b08961e82f991b1a0e",
      "max": 21388428,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_023f93d14c934299be94113aa089ff2f",
      "value": 21388428
     }
    },
    "71dbfe7632524d69b167b4d56d44ed41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb54549aef364adca5e067690cf7258c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd2482a049e843f188adae6ee381101f",
      "placeholder": "​",
      "style": "IPY_MODEL_db323916e13f4d7da295d5ad05cf5c84",
      "value": " 20.4M/20.4M [00:00&lt;00:00, 71.6MB/s]"
     }
    },
    "bd2482a049e843f188adae6ee381101f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c972d82225b84aa3aa6cb513952162b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce8f3ea6a7de46b08961e82f991b1a0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db323916e13f4d7da295d5ad05cf5c84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef0ccb555eb34b45a4c5af40649f68ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
